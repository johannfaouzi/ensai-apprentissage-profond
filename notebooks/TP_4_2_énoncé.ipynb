{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a534ad",
   "metadata": {},
   "source": [
    "**Avant de débuter ce TP** :\n",
    "\n",
    "1. **Changez le type d'exécution sur Google Colab** : `Exécution > Modifiez le type d'exécution > T4 GPU`\n",
    "2. **Installez les paquets ci-dessous** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9274cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install lightning torchmetrics torchinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56480794",
   "metadata": {},
   "source": [
    "3. Exécutez ce code pour supprimer quelques messages et avertissements éventuellement affichés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9000f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"lightning\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"lightning.pytorch.utilities.rank_zero\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"lightning.pytorch.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(\"lightning\")\n",
    "logger.propagate = False\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Missing logger folder.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f04a2",
   "metadata": {},
   "source": [
    "# Synthèse d'images par résonance magnétique cérébrales\n",
    "\n",
    "## Introduction\n",
    "\n",
    "L'imagerie par résonance magnétique (IRM) est une technique d'imagerie médicale permettant d'obtenir des vues en deux ou en trois dimensions de l'intérieur du corps de façon non invasive avec une résolution en contraste relativement élevée ([Wikipedia](https://fr.wikipedia.org/wiki/Imagerie_par_résonance_magnétique)).\n",
    "\n",
    "Il existe différents types d'images par résonance magnétique, appelées pondérations.\n",
    "De manière simplifiée, en modifiant les paramètres d'acquisition, on obtient des images différentes mettant en avant des caractéristiques différentes.\n",
    "\n",
    "Parmi les pondérations existantes, deux sont couramment utilisées :\n",
    "\n",
    "* La **pondération T1** met en avant les régions avec de la graisse, notamment les [tissus adipeux](https://fr.wikipedia.org/wiki/Tissu_adipeux), et donc les régions cervicales contenant beaucoup de tissus adipeux.\n",
    "* La **pondération T2** met en avant les liquides, notamment le [liquide cérébrospinal](https://fr.wikipedia.org/wiki/Liquide_cérébrospinal), et donc les régions cervicales contenant beaucoup de iquide cérébrospinal.\n",
    "\n",
    "Les images cérébrales complètes sont des images en trois dimensions.\n",
    "Néanmoins, la visualisation est plus facile pour l'oeil humain en deux dimensions.\n",
    "On utilise donc en général des coupes en deux dimensions.\n",
    "L'image en trois dimensions étant un parallélépipède rectangle, il existe trois manières de couper une image en trois dimensions, chacune associé à un axe dont la valeur est fixe.\n",
    "\n",
    "[<img src=\"../figures/coupes_irm.png\" width=\"750\" />](../figures/coupes_irm.png)\n",
    "\n",
    "L'image ci-dessus illustre ces trois type de coupe : une coupe axiale (à gauche), une coupe coronnale (au milieu) et une coupe sagittale (à droite).\n",
    "La coupe axiale étant celle qui permet de couvrir le mieux le cerveau, c'est la coupe généralement utilisée.\n",
    "\n",
    "L'image ci-dessous illustre deux coupes axiales, avec une pondération T1 à gauche et une pondération T2 à droite.\n",
    "\n",
    "[<img src=\"../figures/brain_mri_t1_t2.png\" width=\"500\" />](../figures/brain_mri_t1_t2.png)\n",
    "\n",
    "**L'objectif de ce notebook est de prédire l'image T2 à partir de l'image T1**.\n",
    "L'intérêt pratique de cette approche est est de réduire l'utilisation des machines IRM : une seule acquisition serait nécessaire pour obtenir les pondérations.\n",
    "\n",
    "\n",
    "## (Télé)chargement du jeu de données\n",
    "\n",
    "Nous allons travailler sur un jeu de données public appelé [IXI dataset](http://brain-development.org/ixi-dataset/).\n",
    "Les données prétraitées sont disponibles sur ce [dépôt GitHub](https://github.com/Easternwen/IXI-dataset).\n",
    "Dans le dossier `size64`, il y a $1154$ fichiers, correspondant à $2$ images pour chacun des $577$ sujets.\n",
    "La taille de chaque image est $(64, 64)$.\n",
    "\n",
    "La commande suivante permet de cloner le dépôt GitHub dans le dossier `data` nouvellement créé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f56967",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data && cd data && git clone https://github.com/Easternwen/IXI-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25365d",
   "metadata": {},
   "source": [
    "Visualions quelques images du jeu de données grâce à la fonction `plot_images_one_subject()` définie ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa409584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def plot_images_one_subject(idx=None):\n",
    "    \"\"\"Affiche les coupes T1 et T2 d'un sujet.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    idx : int (default = None)\n",
    "        Indice du sujet. La liste des indices valides est disponible\n",
    "        dans la variable `idx_valid` définie ci-dessous. Si l'indice\n",
    "        n'est pas valide, un indice est tiré aléatoirement parmi la\n",
    "        liste d'indices valides.\n",
    "    \"\"\"\n",
    "    idx_valid = [\n",
    "         2,  12,  13,  14,  15,  16,  17,  19,  20,  21,  22,  23,  24,\n",
    "        25,  26,  27,  28,  29,  30,  31,  33,  34,  35,  36,  37,  38,\n",
    "        39,  40,  41,  42,  43,  44,  45,  46,  48,  49,  50,  51,  52,\n",
    "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
    "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
    "        79,  80,  81,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
    "        93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
    "       106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119,\n",
    "       120, 121, 122, 123, 126, 127, 128, 129, 130, 131, 132, 134, 135,\n",
    "       136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 148, 150,\n",
    "       151, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
    "       166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 177, 178, 179,\n",
    "       180, 181, 183, 184, 185, 186, 188, 189, 191, 192, 193, 194, 195,\n",
    "       196, 197, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 209,\n",
    "       210, 211, 212, 213, 214, 216, 217, 218, 219, 221, 222, 223, 224,\n",
    "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238,\n",
    "       239, 240, 241, 242, 244, 246, 247, 248, 249, 250, 251, 252, 253,\n",
    "       254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
    "       267, 268, 269, 270, 274, 275, 276, 277, 278, 279, 280, 282, 284,\n",
    "       285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
    "       298, 299, 300, 302, 303, 304, 305, 306, 307, 308, 310, 311, 312,\n",
    "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 324, 325, 326,\n",
    "       327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 340,\n",
    "       341, 342, 344, 345, 347, 348, 350, 351, 353, 354, 356, 357, 358,\n",
    "       359, 360, 361, 362, 363, 364, 365, 367, 368, 369, 370, 371, 372,\n",
    "       373, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386,\n",
    "       387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399,\n",
    "       400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412,\n",
    "       413, 414, 415, 416, 417, 418, 419, 420, 422, 423, 424, 425, 426,\n",
    "       427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
    "       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
    "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
    "       467, 468, 469, 470, 473, 474, 475, 476, 477, 478, 479, 480, 481,\n",
    "       482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494,\n",
    "       495, 496, 497, 498, 499, 501, 502, 503, 504, 505, 506, 507, 508,\n",
    "       510, 511, 512, 515, 516, 517, 518, 519, 521, 522, 523, 524, 525,\n",
    "       526, 527, 528, 531, 532, 533, 534, 535, 536, 537, 538, 539, 541,\n",
    "       542, 543, 544, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555,\n",
    "       556, 558, 559, 560, 561, 562, 563, 565, 566, 567, 568, 569, 571,\n",
    "       572, 573, 574, 575, 576, 577, 578, 579, 582, 584, 585, 586, 587,\n",
    "       588, 589, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601,\n",
    "       603, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 616, 617,\n",
    "       618, 619, 621, 622, 623, 625, 626, 627, 629, 630, 631, 632, 633,\n",
    "       634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 646, 648,\n",
    "       651, 652, 653, 661, 662\n",
    "    ]\n",
    "\n",
    "    if not (isinstance(idx, int) and idx in idx_valid):\n",
    "        import random\n",
    "        idx = random.choice(idx_valid)\n",
    "\n",
    "    plt.figure(figsize=(9, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(\n",
    "        torch.transpose(\n",
    "            torch.load(\n",
    "                os.path.join(\n",
    "                    'data', 'IXI-dataset', 'size64', f'sub-IXI{idx:03} - T1.pt'\n",
    "                ), weights_only=True\n",
    "            ), 0, 1\n",
    "        ), cmap='gray', origin='lower'\n",
    "    )\n",
    "    plt.title(f\"Coupe T1 pour le sujet {idx:03}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(\n",
    "        torch.transpose(\n",
    "            torch.load(\n",
    "                os.path.join(\n",
    "                    'data', 'IXI-dataset', 'size64', f'sub-IXI{idx:03} - T2.pt'\n",
    "                ), weights_only=True\n",
    "            ), 0, 1), \n",
    "        cmap='gray', origin='lower'\n",
    "    )\n",
    "    plt.title(f\"Coupe T2 pour le sujet {idx:03}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_one_subject(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_one_subject(608)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a747bdb",
   "metadata": {},
   "source": [
    "On va maintenant définir notre propre classe pour ce jeu de données.\n",
    "80% du jeu de données est utilisé pour le jeu d'entraînement, 20% pour le jeu de validation.\n",
    "La classe `CustomDataset()` définie ci-dessous permet de créer ces deux jeux de données.\n",
    "On l'utilise pour créer les *dataloaders* pour les jeux d'entraînement et de validation avec des lots de taille $32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Classe relative au jeu de données IXI.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mode : {'training', 'validation'}\n",
    "        Partie du jeu de données.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, mode=\"training\"):\n",
    "        \n",
    "        root = os.path.join('data', 'IXI-dataset', 'size64')\n",
    "\n",
    "        files = sorted(os.listdir(root))\n",
    "        patient_id = list(set([i.split()[0] for i in files]))\n",
    "        split_idx = int(0.8 * len(patient_id))\n",
    "        imgs = []\n",
    "\n",
    "        if mode == \"training\":\n",
    "            for i in patient_id[:split_idx]:\n",
    "                if (\n",
    "                    os.path.isfile(os.path.join(root, f\"{i} - T1.pt\")) and\n",
    "                    os.path.isfile(os.path.join(root, f\"{i} - T2.pt\"))\n",
    "                ):\n",
    "                    imgs.append((os.path.join(root, f\"{i} - T1.pt\"),\n",
    "                                 os.path.join(root, f\"{i} - T2.pt\")))\n",
    "        elif mode == \"validation\":\n",
    "            for i in patient_id[split_idx:]:\n",
    "                if (\n",
    "                    os.path.isfile(os.path.join(root, f\"{i} - T1.pt\")) and\n",
    "                    os.path.isfile(os.path.join(root, f\"{i} - T2.pt\"))\n",
    "                ):\n",
    "                    imgs.append((os.path.join(root, f\"{i} - T1.pt\"),\n",
    "                                 os.path.join(root, f\"{i} - T2.pt\")))\n",
    "        else:\n",
    "            raise ValueError(\"'mode' doit être l'un de 'training' ou 'validation'.\")\n",
    "\n",
    "        self.imgs = imgs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t1_path, t2_path = self.imgs[index]\n",
    "        \n",
    "        # Ajoute une dimension supplémentaire pour les canaux\n",
    "        t1 = torch.unsqueeze(torch.load(t1_path, weights_only=True), 0).to(dtype=torch.float32)\n",
    "        t2 = torch.unsqueeze(torch.load(t2_path, weights_only=True), 0).to(dtype=torch.float32)\n",
    "\n",
    "        return t1, t2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "\n",
    "# Datasets\n",
    "dataset_train = CustomDataset(mode=\"training\")\n",
    "dataset_val = CustomDataset(mode=\"validation\")\n",
    "\n",
    "# Dataloaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5985891e",
   "metadata": {},
   "source": [
    "On vérifie la taille des images en entrée et en sortie.\n",
    "On n'affiche ici les tailles que pour la première observation du jeu d'entraînement, mais on admettra que ces informations sont valables pour toutes les observations (c'est bien le cas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aaffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Taille d'une image T1 : {dataset_train[0][0].size()}\")\n",
    "print(f\"Taille d'une image T2 : {dataset_train[0][1].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7be6f1",
   "metadata": {},
   "source": [
    "## Générateur\n",
    "\n",
    "Dans cette section, on va entraîner un générateur pour générer l'image T2 à partir de l'image T1.\n",
    "Cela est rendu possible du fait que le jeu de données est apparié : on a accès aux images T1 et T2 du même sujet. \n",
    "\n",
    "### Question 1\n",
    "\n",
    "Complétez les méthodes `__init__()` et `forward()` de la classe `Generator()` définie ci-dessous en utilisant les informations fournies dans le texte ci-dessous.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "Notre générateur aura une architecture de type **U-Net** avec les caractéristiques suivantes :\n",
    "\n",
    "* Les blocs descendants sont constitués d'une couche de convolution, puis d'une couche de normalisation et enfin la fonction d'activation ReLU.\n",
    "\n",
    "* Les blocs ascendants sont constitués d'une couche de convolution transposée, puis d'une couche de normalisation et enfin la fonction d'activation ReLU.\n",
    "\n",
    "L'architecture exacte, avec les valeurs des hyperparamètres pour chacune des couches, est représentée par l'image ci-dessous.\n",
    "Si une couche prend deux cartes de caractéristiques en entrée, celles-ci sont concaténées (sur la dimension des canaux).\n",
    "Le code pour les classes correspondant aux différents blocs vous sont déjà fournies ci-dessous : `UNetDown()`, `UNetUP()` et `FinalBloc()` correspondent aux blocs descendants, blocs ascendants et bloc final respectivement.\n",
    "\n",
    "[<img src=\"../figures/generator.png\" width=\"750\" />](../figures/generator.png)\n",
    "\n",
    "#### Entraînement\n",
    "\n",
    "Le modèle sera entraîné en utilisant l'erreur absolue moyenne comme fonction de perte.\n",
    "Vous pouvez utiliser [torch.nn.L1Loss()](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html).\n",
    "\n",
    "#### Métriques d'évaluation\n",
    "\n",
    "La performance d'un modèle sera évaluée avec la similarité structurelle (SSIM pour [*structural similarity index measure*](https://en.wikipedia.org/wiki/Structural_similarity_index_measure)).\n",
    "Des explications concernant cette métrique seront fournies plus tard dans ce notebook.\n",
    "Pour l'instant, tout ce dont avait besoin de savoir est qu'il s'agit d'un score compris entre $0$ (le moins bon) et $1$ (le meilleur) et qu'elle est déjà implémentée dans le paquet `torchmetrics` : [`torchmetrics.image.StructuralSimilarityIndexMeasure()`](https://lightning.ai/docs/torchmetrics/stable/image/structural_similarity.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d52fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDown(torch.nn.Module):\n",
    "    \"\"\"Bloc descendant de notre U-Net.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Nombre de canaux dans l'image en entrée.\n",
    "        \n",
    "    out_channels : int\n",
    "        Nombre de canaux dans l'image en sortie.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            torch.nn.InstanceNorm2d(out_channels),\n",
    "            torch.nn.ReLU()\n",
    "          )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UNetUp(torch.nn.Module):\n",
    "    \"\"\"Bloc ascendant de notre U-Net.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Nombre de canaux dans l'image en entrée.\n",
    "        \n",
    "    out_channels : int\n",
    "        Nombre de canaux dans l'image en sortie.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            torch.nn.InstanceNorm2d(out_channels),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input=None):\n",
    "        if skip_input is not None:\n",
    "            dim = 0 if x.ndim == 3 else 1\n",
    "            x = torch.cat((x, skip_input), dim)  # add the skip connection\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class FinalBloc(torch.nn.Module):\n",
    "    \"\"\"Bloc final de notre the U-Net.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int\n",
    "        Nombre de canaux dans l'image en entrée.\n",
    "        \n",
    "    out_channels : int\n",
    "        Nombre de canaux dans l'image en sortie.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            torch.nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input=None):\n",
    "        if skip_input is not None:\n",
    "            dim = 0 if x.ndim == 3 else 1\n",
    "            x = torch.cat((x, skip_input), dim)  # add the skip connection\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "\n",
    "\n",
    "class Generator(L.LightningModule):\n",
    "    \"\"\"Classe pour le générateur.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int (default = 1)\n",
    "        Nombre de canaux dans l'image en entrée.\n",
    "\n",
    "    out_channels : int (default = 1)\n",
    "        Nombre de canaux dans l'image renvoyée en sortie.\n",
    "\n",
    "    lr : float (default = 1e-3)\n",
    "        Taux d'apprentissage.\n",
    "\n",
    "    betas : tuple[float, float] (default = (0.9, 0.999))\n",
    "        Paramètres betas de l'algorithme d'optimisation Adam.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, lr=1e-3, betas=(0.9, 0.999)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "\n",
    "        ### BEGIN TODO ###\n",
    "        # Intialisation des blocs de l'architecture\n",
    "        \n",
    "        # Initialisation de la fonction de perte\n",
    "        # self.loss =\n",
    "        \n",
    "        # Initialisation des métriques\n",
    "        # self.ssim_train = \n",
    "        # self.ssim_val = \n",
    "        #### END TODO ####\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### BEGIN TODO ###\n",
    "        # y = \n",
    "        #### END TODO ####\n",
    "        return y\n",
    "    \n",
    "    def step(self, batch, dataset):\n",
    "        \"\"\"Effectue une étape.\n",
    "\n",
    "        Une étape consiste à passer d'un lot d'observations (l'argument batch)\n",
    "        à l'évaluation de la fonction de coût pour ce lot d'observations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : tuple\n",
    "            Un lot d'observations. Le premier élément du tuple est le lot\n",
    "            des entrées, le second est le lot des labels.\n",
    "            \n",
    "        dataset : {\"training\", \"validation\"}\n",
    "            Jeu de données utilisé.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : Tensor, shape = (1,)\n",
    "            La fonction de coût pour ce lot d'observations.\n",
    "        \"\"\"\n",
    "        X, Y = batch \n",
    "        Y_pred = self(X)\n",
    "        loss = self.loss(Y_pred, Y)\n",
    "\n",
    "        if dataset == \"training\":\n",
    "            metric = self.ssim_train\n",
    "            name = \"train\"\n",
    "            bar_step = True\n",
    "        else:\n",
    "            metric = self.ssim_val\n",
    "            name = \"val\"\n",
    "            bar_step = False\n",
    "\n",
    "        ssim = metric(Y_pred, Y)\n",
    "        self.log(f\"loss_{name}\", loss, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "        self.log(f\"ssim_{name}\", ssim, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        return self.step(batch, \"training\")\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        return self.step(batch, \"validation\")\n",
    "    \n",
    "    def on_train_start(self):\n",
    "        \"\"\"Code exécuté au début de l'entraînement.\"\"\"\n",
    "        string = f\"Version {self.trainer.logger.version}\"\n",
    "        print(f\"{string}\\n{'=' * len(string)}\\n\")\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Code exécuté à la fin de chaque époque d'entraînement.\"\"\"\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        string = (f\"\"\"\n",
    "            Époque {self.trainer.current_epoch + 1} / {self.trainer.max_epochs}\n",
    "            -----------------------------------------------\n",
    "            |     Jeu      | Fonction de perte |   SSIM   |\n",
    "            | ------------ | ----------------- | -------- |\n",
    "            | Entraînement |{metrics['loss_train'].item():^19.5f}|{metrics['ssim_train'].item():^10.6f}|\n",
    "            |  Validation  |{metrics['loss_val'].item():^19.5f}|{metrics['ssim_val'].item():^10.6f}|\n",
    "            -----------------------------------------------\n",
    "        \"\"\")\n",
    "        string = '\\n'.join([line.strip() for line in string.strip().split('\\n')])\n",
    "        print(string, \"\\n\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure l'algorithme d'optimisation à utiliser.\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, betas=self.betas)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7ff93",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Affichez un résumé de l'architecture définie dans la classe ci-dessus.\n",
    "De combien de paramètres entraînables cette architecture est-elle constituée ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b18747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527db33b",
   "metadata": {},
   "source": [
    "**Réponse** : TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d921178d",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Entraînez votre modèle pendant $20$ époques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "model_generator = Generator()\n",
    "\n",
    "trainer_generator = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    enable_model_summary=False,  # supprimer le résumé du modèle\n",
    "    logger=CSVLogger('.'),  # sauvegarder les résultats dans un fichier CSV\n",
    "    num_sanity_val_steps=0,  # ne pas effectuer d'étape de validation avant l'entraînement\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=5)]  # mettre à jour la barre de progression tous les 10 lots\n",
    ")\n",
    "\n",
    "trainer_generator.fit(\n",
    "    model=model_generator,\n",
    "    train_dataloaders=dataloader_train,\n",
    "    val_dataloaders=dataloader_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135381e",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Utilisez la fonction `plot_generated_images()` définie ci-dessous pour afficher quelques images T2 générées par le modèle sur le jeu de validation et les comparer avec les vraies images T2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d276f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_generated_images(model, dataset, n_images=5):\n",
    "    \"\"\"Affiche des images générées par le modèle.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        Modèle.\n",
    "    \n",
    "    dataset : torch.nn.utils.Dataset\n",
    "        Jeu de données.\n",
    "    \n",
    "    n_images : int (default = 10)\n",
    "        Nombres d'images à générer.\n",
    "    \n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if n_images > len(dataset):\n",
    "        n_images = len(dataset)\n",
    "    \n",
    "    idx = torch.multinomial(torch.ones(len(dataset)), n_images)\n",
    "    n_rows = n_images\n",
    "\n",
    "    X = torch.stack([dataset[i][0] for i in idx])\n",
    "    Y = torch.stack([dataset[i][1] for i in idx])\n",
    "    Y_pred = model(X).detach()\n",
    "    \n",
    "    X = torch.transpose(X, 2, 3)\n",
    "    Y = torch.transpose(Y, 2, 3)\n",
    "    Y_pred = torch.transpose(Y_pred, 2, 3)\n",
    "\n",
    "    fig, ax = plt.subplots(n_rows, 3, figsize=(9, n_rows * 3))\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        sujet_idx = idx[i]\n",
    "        \n",
    "        ax[i, 0].imshow(X[i, 0], cmap='gray', origin='lower')\n",
    "        ax[i, 0].set_xticks([]); ax[i, 0].set_yticks([]);\n",
    "        ax[i, 0].set_ylabel(f'Sujet {sujet_idx}', fontsize=14)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax[i, 0].set_title(f'Coupe T1 originale', fontsize=14)\n",
    "            ax[i, 1].set_title(f'Coupe T2 originale', fontsize=14)\n",
    "            ax[i, 2].set_title(f'Coupe T2 générée', fontsize=14)\n",
    "        \n",
    "        ax[i, 1].imshow(Y[i, 0], cmap='gray', origin='lower')\n",
    "        ax[i, 1].set_xticks([]); ax[i, 1].set_yticks([]);\n",
    "        \n",
    "        ax[i, 2].imshow(Y_pred[i, 0], cmap='gray', origin='lower')\n",
    "        ax[i, 2].set_xticks([]); ax[i, 2].set_yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439819f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acd3914",
   "metadata": {},
   "source": [
    "## Évaluation de la qualité des images\n",
    "\n",
    "### Limites de l'erreur quadratique (ou absolue) moyenne\n",
    "\n",
    "Les métriques pour comparer deux images qui effectuent une moyenne de comparaisons pixel par pixel, telles que l'erreur quadratique moyenne et l'erreur absolue moyenne, ne sont pas forcément les plus pertinentes par rapport à la perception visuelle humaine.\n",
    "*Une image valant mille mots*, l'image ci-dessous illustre ce propos :\n",
    "\n",
    "[<img src=\"../figures/comparaison_mse_ssim.png\" width=\"750\" />](../figures/comparaison_mse_ssim.png)\n",
    "\n",
    "L'image originale se trouve en haut à gauche, et les cinq autres images sont des versions modifiées de cette image caractérisées par (quasiment) la même erreur quadratique moyenne ($144$ avec les valeurs des pixels allant de $0$ à $255$).\n",
    "Pourtant, pour la perception humaine, certaines images modifiées sont bien plus proches de l'image originale que d'autres.\n",
    "\n",
    "### Indice de mesure de similarité structurelle (*Structural Similarity Index Measure*)\n",
    "\n",
    "Une autre métrique possible pour comparer deux images est l'**indice de mesure de similarité structurelle** (SSIM pour *Structural Similarity Index Measure*).\n",
    "L'indice de mesure de similarité structurelle combine trois caractéristiques pour comparer deux images : la **luminosité**, le **contraste** et la **similarité structurelle**.\n",
    "\n",
    "Ces caractéristiques se calculent à partir de métriques simples sur les deux images, telles que leur moyenne et leur écart-type.\n",
    "On suppose que les valeurs des pixels ont été normalisées dans l'intervalle $[0, 1]$.\n",
    "On utilise les notations suivantes :\n",
    "* $x$ et $y$ sont deux images.\n",
    "* $\\mu$ correspond à la valeur moyenne des pixels : $\\mu_x$ pour l'image $x$ et $\\mu_y$ pour l'image $y$.\n",
    "* $\\sigma$ correspond à l'écart-type des pixels : $\\sigma_x$ pour l'image $x$ et $\\sigma_y$ pour l'image $y$.\n",
    "* $\\sigma_{xy}$ correspond à la covariance entre $x$ et $y$.\n",
    "\n",
    "La **luminosité** correspond à la moyenne des pixels. Pour comparer les luminosités de deux images, on utilise la formule suivante :\n",
    "$$\n",
    "l(x, y) = \\frac{2 \\mu_x \\mu_y + C_1}{\\mu_x^2 + \\mu_y^2 + C_1}\n",
    "$$\n",
    "où $C_1$ est une constante positive pour éviter une division par un nombre trop faible ($C_1 = 0.01$ par défaut).\n",
    "Le score est bien positif (le numérateur et le dénominateur le sont), est inférieur ou égal à 1 et est égal à 1 si et seulement si les deux moyennes sont égales :\n",
    "$$\n",
    "(\\mu_x - \\mu_y)^2 \\geq 0 \\implies \\mu_x^2 - 2 \\mu_x \\mu_y + \\mu_y^2 \\geq 0 \\implies 2 \\mu_x \\mu_y \\leq \\mu_x^2 + \\mu_y^2 \\implies 2 \\mu_x \\mu_y + C_1 \\leq \\mu_x^2 + \\mu_y^2 + C_1 \n",
    "$$\n",
    "\n",
    "Le **contraste**, qui est une mesure de la dispersion des pixels, correspond à l'écart-type des pixels. Pour comparer les contrastes de deux images, on utilise la même formule que pour la luminosité en remplaçant les moyennes par les écart-types :\n",
    "$$\n",
    "c(x, y) = \\frac{2 \\sigma_x \\sigma_y + C_1}{\\sigma_x^2 + \\sigma_y^2 + C_2}\n",
    "$$\n",
    "où $C_2$ est une constante positive pour éviter une division par un nombre trop faible ($C_2 = 0.03$ par défaut).\n",
    "Le score est bien positif (le numérateur et le dénominateur le sont), est inférieur ou égal à 1 et est égal à 1 si et seulement si les deux moyennes sont égales :\n",
    "$$\n",
    "(\\sigma_x - \\sigma_y)^2 \\geq 0 \\implies \\sigma_x^2 - 2 \\sigma_x \\sigma_y + \\sigma_y^2 \\geq 0 \\implies 2 \\sigma_x \\sigma_y \\leq \\sigma_x^2 + \\sigma_y^2 \\implies 2 \\sigma_x \\sigma_y + C_2 \\leq \\sigma_x^2 + \\sigma_y^2 + C_2 \n",
    "$$\n",
    "\n",
    "La **similarité structurelle**, qui mesure la corrélation entre deux images, correspond au coefficient de corrélation entre les deux images :\n",
    "$$\n",
    "s(x, y) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x \\sigma_y + C_3}\n",
    "$$\n",
    "où $C_3$ est une constante positive pour éviter une division par un nombre trop faible  ($C3=0.015$ par défaut).\n",
    "Le score est compris entre $0$ et $1$ et vaut $1$ si et seulement si les deux images sont égales.\n",
    "\n",
    "L'**indice de mesure de similarité structurelle** combine ces trois caractéristiques avec la formule suivante :\n",
    "$$\n",
    "\\text{SSIM}(x, y) = \\left[ l(x, y) \\right]^\\alpha \\cdot \\left[ c(x, y) \\right]^\\beta \\cdot \\left[ s(x, y) \\right]^\\gamma\n",
    "$$\n",
    "où les exposants $\\alpha$, $\\beta$ et $\\gamma$ indiquent les poids des trois variables dans le score final.\n",
    "\n",
    "Avec les valeurs par défaut ($\\alpha = \\beta = \\gamma = 1$) et $C_3 = C_2 / 2$, on obtient la formule simplifiée suivante :\n",
    "$$\n",
    "\\text{SSIM}(x, y) = \\frac{(2 \\mu_x \\mu_y + C_1)(2 \\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1)(\\sigma_x^2 + \\sigma_y^2 + C_2)}\n",
    "$$\n",
    "\n",
    "En revenant aux images ci-dessus, on voit bien que les images modifiées qui nous semblent davantage différentes de l'image originale ont des indices plus faibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d65d54",
   "metadata": {},
   "source": [
    "## Réseau antagoniste génératif conditionnel (cGAN)\n",
    "\n",
    "Un réseau antagoniste génératif (GAN pour *Generative Adversarial Network*) est une architecture composée de deux réseaux de neurones :\n",
    "* Le **générateur** génère des observations synthétiques.\n",
    "* Le **discriminateur** sépare les vraies observations des observations synthétiques.\n",
    "\n",
    "Un réseau antagoniste génératif conditionnel (cGAN) est un réseau antagoniste génératif où le générateur et le discriminateur sont conditionnés :\n",
    "* Le générateur ne doit pas seulement générer une observation synthétique réaliste, il doit en générer une en lien avec la condition.\n",
    "* Le discriminateur ne doit pas seulement déterminer si l'observation est vraie ou synthétique, il doit également déterminer si l'observation est en lien avec la condition générée.\n",
    "En effet, le générateur ne doit pas générer n'importe quelle image T2 à partir de l'image T1 (la condition) en entrée.\n",
    "Il doit générer l'image T2 correspond à l'image T1 en entrée et non à n'importe quelle image T1.\n",
    "\n",
    "Ayant déjà défini une classe pour l'architecture du générateur, il nous reste à définir le discriminateur.\n",
    "L'image ci-dessous décrit l'architecture que l'on va utiliser pour le discriminateur :\n",
    "\n",
    "[<img src=\"../figures/discriminator.png\" width=\"750\" />](../figures/discriminator.png)\n",
    "\n",
    "### Question 5\n",
    "\n",
    "La classe `Discriminator()` définie ci-dessous implémente cette architecutre.\n",
    "L'architecture prend en entrée deux images et renvoie une probabilité.\n",
    "Complétez le code manquant dans les méthodes `__init__()` et `forward()` de la classe `Discriminator()` en vous aidant de la méthode statique `discriminator_block()`.\n",
    "\n",
    "> **Remarque** : Il manque sur l'image la couche d'aplatissement ([`torch.nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html)) à la fin du discriminateur pour transformer l'image de taille $(1, 1, 1)$, c'est-à-dire un seul pixel avecun seul canal, en un vecteur de taille $(1,)$.\n",
    "\n",
    "> **Remarque** : On a légèrement complexifié le code de la classe pour la rendre plus flexible. Avec cette implémentation, le discriminateur peut prendre en entrée soit une image (discriminateur non-conditionnel) soit deux images (discriminateur conditionnel) qui sont concaténées sur la dimension des canaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab1db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(L.LightningModule):\n",
    "    \"\"\"Classe pour le discriminateur.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int (default = 1)\n",
    "        Le nombre de canaux dans l'image en entrée.\n",
    "        \n",
    "    condition : bool (default = True)\n",
    "        Indique si le discriminateur est conditionnel ou pas.\n",
    "        Si True, le discriminateur est conditionnel et les deux\n",
    "        images (l'entrée et la condition) sont concaténées le\n",
    "        long de l'axe des canaux.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, condition=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.condition = (condition is True)\n",
    "\n",
    "        ### BEGIN TODO ###\n",
    "        \n",
    "        #### END TODO ####\n",
    "\n",
    "    def forward(self, T1, T2):\n",
    "        if T2 is not None:\n",
    "            assert self.condition\n",
    "            dim = 1 if T1.ndim == 4 else 3\n",
    "            T1 = torch.concat([T1, T2], dim=dim)\n",
    "        else:\n",
    "            assert not self.condition\n",
    "\n",
    "        ### BEGIN TODO ###\n",
    "        # y = \n",
    "        #### END TODO ####\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def discriminator_block(in_channels, out_channels):\n",
    "        \"\"\"Renvoie un bloc convolutif du discriminateur.\"\"\"\n",
    "        return [\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            torch.nn.LeakyReLU()\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc02ac9",
   "metadata": {},
   "source": [
    "On affiche maintenant un résumé de l'architecture de de notre discriminateur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Discriminator(), input_size=[(32, 1, 64, 64), (32, 1, 64, 64)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf01bf1",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Maintenant que l'on a les classes pour le générateur et le discriminateur, on peut créer la classe implémentant notre réseau antagoniste génératif conditionnel.\n",
    "La classe `CGAN()` définie ci-dessous implémente cette architecture de réseaux antagonistes génératifs conditionnels.\n",
    "Complétez les méthodes `__init__()` et `forward()` de la classe `CGAN()` en utilisant les informations fournies dans le texte ci-dessous.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "On a déjà défini les classes pour le générateur et le discriminateur, il n'y a plus rien à faire à part initialiser ces deux réseaux de neurones.\n",
    "\n",
    "#### Entraînement\n",
    "\n",
    "On va entraîner en alternance le générateurs et les discriminateur, afin que l'un incite l'autre à s'améliorer :\n",
    "\n",
    "* Concernant le **discriminateur**, il s'agit d'un problème classique de classification binaire. Un lot d'observations renvoie un lot de vraies images T1 et un lot de vraies images T2. On utilise le générateur pour obtenir les images T2 synthétiques. On effectue ensuite l'étape suivante :\n",
    "    + On fournit en entrée du discriminateur les vraies images T1 et les vraies images T2, pour obtenir les logits que les vraies images T2 soient vraies. On fournit ces logits en entrée de la fonction de coût avec des labels positifs, soit $1$ (car les images T2 sont vraies ici).\n",
    "    + On fournit en entrée du discriminateur les vraies images T1 et les images T2 synthétiques, pour obtenir les logits que les images synthétiques T2 soient vraies. On fournit ces logits en entrée de la fonction de coût avec des labels négatifs, soit $0$ (car les images T2 sont synthétiques ici). Pour stabiliser l'entraînement, **on utilise l'erreur quadratique moyenne (*a.k.a. MSE loss*) au lieu de l'entropie croisée binaire**.\n",
    "\n",
    "* Concernant le **générateur**, la fonction de coût a deux composantes : celle correspondant à la différence entre l'image T2 synthétique et la vraie image T2, et celle correspondant à la duperie du discriminateur. Un lot d'observations renvoie un lot de vraies images T1 et un lot de vraies images T2. On utilise le générateur pour obtenir les images T2 synthétiques. Pour la première composante, on utilise l'erreur absolue moyenne (*a.k.a. L1 loss*) pour mesurer la différence entre l'image T2 synthétique et la vraie image T2. Pour la seconde composante, c'est également un problème de classification binaire. On fournit en entrée du discriminateur les vraies images T1 et les images T2 synthétiques, pour obtenir les logits que les images synthétiques T2 soient vraies. On fournit ces logits en entrée de la fonction de coût avec des labels positifs, soit $1$ (car on veut duper le discriminateur, c'est-à-dire qu'ils classent les images T2 synthétiques comme des vraies). On utilise à nouveau l'erreur quadratique moyenne comme fonction de coût.\n",
    "\n",
    "#### Évaluation\n",
    "\n",
    "Le principal critère d'évaluation reste la qualité des images synthétiques générées par le générateur.\n",
    "On utilise à nouveau la similarité structurelle pour comparer les images synthétiques avec les vraies images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af8e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN(L.LightningModule):\n",
    "    \"\"\"Classe implémentant notre réseau antagoniste génératif conditionnel (cGAN).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : int (default = 1)\n",
    "        Nombre de canaux dans l'image en entrée.\n",
    "\n",
    "    out_channels : int (default = 1)\n",
    "        Nombre de canaux dans l'image renvoyée en sortie.\n",
    "\n",
    "    lr : float (default = 1e-3)\n",
    "        Taux d'apprentissage.\n",
    "\n",
    "    betas : tuple[float, float] (default = (0.9, 0.999))\n",
    "        Paramètres betas de l'algorithme d'optimisation Adam.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, out_channels=1, lr=1e-3, betas=(0.9, 0.999)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "\n",
    "        ### BEGIN TODO ###\n",
    "        # Initialisation des générateurs et des discriminateurs\n",
    "        # self.generator = \n",
    "        # self.discriminator = \n",
    "\n",
    "        # Initialisation des fonctions de perte\n",
    "        # self.classification_loss = \n",
    "        # self.reconstruction_loss = \n",
    "        #### END TODO ####\n",
    "\n",
    "        # Initialisation des métriques\n",
    "        self.ssim_train = StructuralSimilarityIndexMeasure(data_range=(-1.0, 1.0))\n",
    "        self.ssim_val = StructuralSimilarityIndexMeasure(data_range=(-1.0, 1.0))\n",
    "\n",
    "        # Désactive l'optimisation automatique\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.generator(x)\n",
    "    \n",
    "    def discriminator_step(self, T1, T2, dataset):\n",
    "        \"\"\"Effectue une étape pour le discriminateur.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        T1 : Tensor\n",
    "            Vraies images T1.\n",
    "        \n",
    "        T2 : Tensor\n",
    "            Vraies images T2.\n",
    "            \n",
    "        dataset : {'training', 'validation'}\n",
    "            Type d'étape (entraînement ou validation).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.float32\n",
    "            Fonction de coût.\n",
    "        \n",
    "        \"\"\"\n",
    "        ### BEGIN TODO ###\n",
    "        # Génère des images T2 synthétiques à partir des vraies images T1\n",
    "        # T2_fake = \n",
    "        \n",
    "        # Calcule les logits des images T2 vraies et synthétiques sachant la vraie image T1\n",
    "        \n",
    "        # Génère les tenseurs pour les labels vrais (1) et synthétiques (0)\n",
    "        \n",
    "        # Calcule la fonction de coût\n",
    "        # loss = \n",
    "        #### END TODO ####\n",
    "\n",
    "        if dataset == \"training\":\n",
    "            metric = self.ssim_train\n",
    "            name = \"train\"\n",
    "            bar_step = True\n",
    "        else:\n",
    "            metric = self.ssim_val\n",
    "            name = \"val\"\n",
    "            bar_step = False\n",
    "\n",
    "        ssim = metric(T2_fake, T2)\n",
    "        self.log(f\"loss_discriminator_{name}\", loss, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "        self.log(f\"ssim_discriminator_{name}\", ssim, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def generator_step(self, T1, T2, dataset):\n",
    "        \"\"\"Effectue une étape pour les générateur.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T1 : Tensor\n",
    "            Vraies images T1.\n",
    "\n",
    "        T2 : Tensor\n",
    "            Vraies images T2.\n",
    "\n",
    "        dataset : {'training', 'validation'}\n",
    "            Type d'étape (entraînement ou validation).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.float32\n",
    "            Fonction de coût.\n",
    "        \"\"\"\n",
    "        ### BEGIN TODO ###\n",
    "        # Génère des images T2 synthétiques à partir des vraies images T1\n",
    "        # T2_fake = \n",
    "        \n",
    "        # Calcule les logits des images T1 et T2 synthétiques\n",
    "        \n",
    "        # Génère les tenseurs pour les labels vrais (1)\n",
    "        \n",
    "        # Calcule la fonction de coût\n",
    "        # loss = \n",
    "        #### END TODO ####\n",
    "\n",
    "        if dataset == \"training\":\n",
    "            metric = self.ssim_train\n",
    "            name = \"train\"\n",
    "            bar_step = True\n",
    "        else:\n",
    "            metric = self.ssim_val\n",
    "            name = \"val\"\n",
    "            bar_step = False\n",
    "\n",
    "        ssim = metric(T2_fake, T2)\n",
    "        self.log(f\"loss_generator_{name}\", loss, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "        self.log(f\"ssim_generator_{name}\", ssim, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        T1, T2 = batch\n",
    "        optimizer_generator, optimizer_discriminator = self.optimizers()\n",
    "        \n",
    "        # Entraînement du générateur\n",
    "        optimizer_generator.zero_grad()\n",
    "        loss_generator = self.generator_step(T1, T2, \"training\")\n",
    "        loss_generator.backward()\n",
    "        optimizer_generator.step()\n",
    "\n",
    "        # Entraînement du discriminateurs\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        loss_discriminator = self.discriminator_step(T1, T2, \"training\")\n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "        \n",
    "        return loss_generator + loss_discriminator\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        T1, T2 = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Entraînement des générateurs\n",
    "            loss_generator = self.generator_step(T1, T2, \"validation\")\n",
    "\n",
    "            # Entraînement des discriminateurs\n",
    "            loss_discriminator = self.discriminator_step(T1, T2, \"validation\")\n",
    "\n",
    "        return loss_generator + loss_discriminator\n",
    "        \n",
    "    def on_train_start(self):\n",
    "        \"\"\"Code exécuté au début de l'entraînement.\"\"\"\n",
    "        string = f\"Version {self.trainer.logger.version}\"\n",
    "        print(f\"{string}\\n{'=' * len(string)}\\n\")\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Code exécuté à la fin de chaque époque d'entraînement.\"\"\"\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        \n",
    "        loss_gen_train = metrics[\"loss_generator_train\"].item()\n",
    "        loss_dis_train = metrics[\"loss_discriminator_train\"].item()\n",
    "        ssim_gen_train = metrics[\"ssim_generator_train\"].item()\n",
    "        ssim_dis_train = metrics[\"ssim_discriminator_train\"].item()\n",
    "        \n",
    "        loss_gen_val = metrics[\"loss_generator_val\"].item()\n",
    "        loss_dis_val = metrics[\"loss_discriminator_val\"].item()\n",
    "        ssim_gen_val = metrics[\"ssim_generator_val\"].item()\n",
    "        ssim_dis_val = metrics[\"ssim_discriminator_val\"].item()\n",
    "\n",
    "        string = (f\"\"\"\n",
    "            Époque {self.trainer.current_epoch + 1} / {self.trainer.max_epochs}\n",
    "            -----------------------------------------------------------------\n",
    "            |              Jeu               | Fonction de perte |   SSIM   |\n",
    "            | ------------------------------ | ----------------- | -------- |\n",
    "            |   Entraînement (générateurs)   |{loss_gen_train:^19.5f}|{ssim_gen_train:^10.6f}|\n",
    "            | Entraînement (discriminateurs) |{loss_dis_train:^19.5f}|{ssim_dis_train:^10.6f}|\n",
    "            |    Validation (générateurs)    |{loss_gen_val:^19.5f}|{ssim_gen_val:^10.6f}|\n",
    "            |  Validation (discriminateurs)  |{loss_dis_val:^19.5f}|{ssim_dis_val:^10.6f}|\n",
    "            -----------------------------------------------------------------\n",
    "        \"\"\")\n",
    "        string = '\\n'.join([line.strip() for line in string.strip().split('\\n')])\n",
    "        print(string, \"\\n\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure les algorithmes d'optimisation à utiliser.\"\"\"\n",
    "        optimizer_generator = torch.optim.Adam(\n",
    "            self.generator.parameters(), lr=self.lr, betas=self.betas\n",
    "        )\n",
    "\n",
    "        optimizer_discriminator = torch.optim.Adam(\n",
    "            self.discriminator.parameters(), lr=self.lr, betas=self.betas\n",
    "        )\n",
    "        return [optimizer_generator, optimizer_discriminator]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9bf3b",
   "metadata": {},
   "source": [
    "On entraîne maintenant notre cGAN pendant $20$ époques.\n",
    "Notez que, par rapport au générateur seul, **on utilise un taux d'apprentissage plus faible**.\n",
    "Cela est nécessaire pour que le discriminateur ne devienne pas bien meilleur que le générateur, ce qui empêcherait le générateur de devenir meilleur (c'est notre objectif principal, le discriminateur n'est qu'un outil pour aider le générateur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947574ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "model_cgan = CGAN(lr=1e-4)\n",
    "\n",
    "trainer_cgan = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    enable_model_summary=False,  # supprimer le résumé du modèle\n",
    "    logger=CSVLogger('.'),  # sauvegarder les résultats dans un fichier CSV\n",
    "    num_sanity_val_steps=0,  # ne pas effectuer d'étape de validation avant l'entraînement\n",
    "    log_every_n_steps=10,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=5)]  # mettre à jour la barre de progression tous les 10 lots\n",
    ")\n",
    "\n",
    "trainer_cgan.fit(\n",
    "    model=model_cgan,\n",
    "    train_dataloaders=dataloader_train,\n",
    "    val_dataloaders=dataloader_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1dcb7",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Visualizez quelques images générées par le générateur du cGAN grâce à la fonction `plot_generated_images()` définie plus haut.\n",
    "Comparez les performances du générateur du cGAN avec le générateur et expliquez la différence.\n",
    "Suggérez des pistes d'amélioration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2457092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc280b",
   "metadata": {},
   "source": [
    "**Réponse** : TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed62b95",
   "metadata": {},
   "source": [
    "## CycleGAN\n",
    "\n",
    "Un réseau antagoniste génératif cyclique (CycleGAN) est un modèle de traduction d'images **non supervisée**.\n",
    "Cette approche se base sur deux réseaux antagonistes génératifs et est entraîné sur des **données non-appariées**.\n",
    "On ne va donc plus utiliser cette propriété de notre jeu de données et faire comme si les données étaient non-appariées.\n",
    "\n",
    "La traduction d'images est une méthode générative **conditionnelle**.\n",
    "On ne veut pas obtenir n'importe quelle image en sortie, mais une image en lien avec l'image fournie en entrée.\n",
    "Quand on a accès à des données appartiées, c'est-à-dire l'image originale et sa traduction, on peut entraîner un modèle génératif conditionnel comme on l'a fait dans la première partie de ce notebook.\n",
    "Néanmoins, cette approche est impossible si on n'a pas accès à des données appariées, ce qui est souvent le cas.\n",
    "\n",
    "Pour contrer ce problème, l'innovation principale des *CycleGAN* est d'utiliser **deux réseaux antagonistes génératifs** (GAN) et d'introduire le concept de **cohérence des cycles** : si on génère une image synthétique dans le domaine *Y* à partir d'une image originale dans le domaine *X*, puis si on génère une image synthétique dans le domaine *X* à partir de l'image synthétique dans le domaine *Y*, alors les deux images dans le domaine *X* (l'image originale et l'image générée) doivent être très proches.\n",
    "En effet, si l'image générée dans le domaine *Y* ne contient pas les caractéristiques de l'image originale, alors il sera impossible d'avoir une image générée dans le domaine *X*, générée à partir de l'image générée dans le domaine *Y*, proche de l'image originale.\n",
    "\n",
    "L'image ci-dessous résume les principaux concepts d'un CycleGAN :\n",
    "\n",
    "[<img src=\"../figures/cycle_consistency.png\" width=\"750\" />](../figures/cycle_consistency.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b62653",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "La classe `CycleGAN()` définie ci-dessous implémente cette architecture de réseaux antagonistes génératifs cycliques.\n",
    "Complétez les méthodes `__init__()`, `discriminators_step()` et `generators_step()` de la classe `CycleGAN()` en utilisant les informations fournies dans le texte ci-dessous.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "Les deux réseaux ont la même architecture, à savoir :\n",
    "\n",
    "* Pour les générateurs, on utilise l'architecture déjà définie dans la classe `Generator()`.\n",
    "\n",
    "* Pour les discriminateurs, on utilise l'architecture définie dans le classe `Discriminator()`.\n",
    "\n",
    "#### Entraînement\n",
    "\n",
    "On va entraîner en alternance les générateurs et les discriminateurs, afin que les uns incitent les autres à s'améliorer :\n",
    "\n",
    "* Concernant les **discriminateurs**, il s'agit de deux problèmes classiques de classification binaire. Un lot d'observations renvoie un lot de vraies images T1 et un lot de vraies images T2. On va également utiliser les deux générateurs pour obtenir un lot d'images T1 synthétiques et un lot d'images T2 synthétiques. On va fournir les images T1 vraies et synthétiques en entrée du discriminateur T1 pour obtenir ses prédictions (passe avant), évaluer la fonction de perte pour calculer les gradients (passe arrière) pour mettre à jour les valeurs des paramètres entraînables du discriminateur T1. On effectue de même pour le discriminateur T2. Cependant, on va utiliser une autre fonction de perte que l'entropie croisée. En effet, une technique pour stabiliser l'entraînement et générer de meilleures images est de **remplacer l'entropie croisée par l'erreur quadratique moyenne** (*a.k.a. MSE loss*).\n",
    "\n",
    "* Concernant les **générateurs**, l'entraînement est un peu plus complexe. On a tout d'abord la partie qui consiste à tromper les discriminateurs, c'est-à-dire le même problème de classification binaire que pour les discriminateurs mais avec les labels inversés : les labels doivent être *vrais* pour les images générées et *générées* pour les vraies images. Enfin, il faut rajouter les termes pour la cohérence des cycles : on utilisera l'erreur absolue moyenne (*a.k.a. L1 loss*) entre la vraie image et l'image synthétique (dans le même domaine).\n",
    "\n",
    "#### Évaluation\n",
    "\n",
    "Le principal critère d'évaluation reste la qualité des images synthétiques générées par le générateur du domaine $T1$ vers le domaine $T2$.\n",
    "On utilise à nouveau la similarité structurelle pour comparer les images synthétiques avec les vraies images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da654d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "class CycleGAN(L.LightningModule):\n",
    "    def __init__(self, in_channels=1, out_channels=1, lr=1e-3, betas=(0.9, 0.999)):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        \n",
    "        ### BEGIN TODO ###\n",
    "        # Initialisation des générateurs et des discriminateurs\n",
    "        # self.generator_from_t1_to_t2 = \n",
    "        # self.generator_from_t2_to_t1 = \n",
    "        # self.discriminator_t1 = \n",
    "        # self.discriminator_t2 = \n",
    "        \n",
    "        # Initialisation des fonctions de perte\n",
    "        #### END TODO ####\n",
    "\n",
    "        # Initialisation des métriques\n",
    "        self.ssim_train = StructuralSimilarityIndexMeasure(data_range=(-1.0, 1.0))\n",
    "        self.ssim_val = StructuralSimilarityIndexMeasure(data_range=(-1.0, 1.0))\n",
    "        \n",
    "        # Désactive l'optimisation automatique\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.generator_from_t1_to_t2(x)\n",
    "        \n",
    "    def discriminators_step(self, T1, T2, dataset):\n",
    "        \"\"\"Effectue une étape pour les discriminateurs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        T1 : Tensor\n",
    "            Vraies images T1.\n",
    "        \n",
    "        T2 : Tensor\n",
    "            Vraies images T2.\n",
    "            \n",
    "        dataset : {'training', 'validation'}\n",
    "            Type d'étape (entraînement ou validation).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.float32\n",
    "            Fonction de coût.\n",
    "        \n",
    "        \"\"\"\n",
    "        ### BEGIN TODO ###\n",
    "        # Génère des images T1 et T2 synthétiques à partir des vraies images T1 et T2 \n",
    "        # T2_fake = \n",
    "        # T1_fake = \n",
    "        \n",
    "        # Calcule les logits des images T1 et T2 vraies et synthétiques\n",
    "        \n",
    "        # Génère les tenseurs pour les labels vrais (1) et synthétiques (0)\n",
    "        \n",
    "        # Calcule la fonction de coût\n",
    "        # loss = \n",
    "        #### END TODO ####\n",
    "\n",
    "        if dataset == 'training':\n",
    "            metric = self.ssim_train\n",
    "            name = 'train'\n",
    "            bar_step = True\n",
    "        else:\n",
    "            metric = self.ssim_val\n",
    "            name = 'val'\n",
    "            bar_step = False\n",
    "\n",
    "        ssim = metric(T2_fake, T2)\n",
    "        self.log(f\"loss_discriminators_{name}\", loss, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "        self.log(f\"ssim_discriminators_{name}\", ssim, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def generators_step(self, T1, T2, dataset):\n",
    "        \"\"\"Effectue une étape pour les générateurs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        T1 : Tensor\n",
    "            Vraies images T1.\n",
    "        \n",
    "        T2 : Tensor\n",
    "            Vraies images T2.\n",
    "            \n",
    "        dataset : {'training', 'validation'}\n",
    "            Type d'étape (entraînement ou validation).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.float32\n",
    "            Fonction de coût.\n",
    "        \n",
    "        \"\"\"\n",
    "        ### BEGIN TODO ###\n",
    "        # Génère des images T1 et T2 synthétiques à partir des vraies images T1 et T2 \n",
    "        # T2_fake = \n",
    "        # T1_fake = \n",
    "        \n",
    "        # Génère des images T1 et T2 synthétiques à partir des images synthétiques T1 et T2\n",
    "\n",
    "        # Calcule les logits des images T1 et T2 synthétiques\n",
    "\n",
    "        # Génère les tenseurs pour les labels vrais (1)\n",
    "        \n",
    "        # Calcule la fonction de coût\n",
    "        # loss = \n",
    "        #### END TODO ####\n",
    "\n",
    "        if dataset == \"training\":\n",
    "            metric = self.ssim_train\n",
    "            name = \"train\"\n",
    "            bar_step = True\n",
    "        else:\n",
    "            metric = self.ssim_val\n",
    "            name = \"val\"\n",
    "            bar_step = False\n",
    "\n",
    "        ssim = metric(T2_fake, T2)\n",
    "        self.log(f\"loss_generators_{name}\", loss, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "        self.log(f\"ssim_generators_{name}\", ssim, prog_bar=bar_step, on_step=bar_step, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        T1, T2 = batch\n",
    "        optimizer_generators, optimizer_discriminators = self.optimizers()\n",
    "        \n",
    "        # Entraînement des générateurs\n",
    "        optimizer_generators.zero_grad()\n",
    "        loss_generators = self.generators_step(T1, T2, \"training\")\n",
    "        loss_generators.backward()\n",
    "        optimizer_generators.step()\n",
    "\n",
    "        # Entraînement des discriminateurs\n",
    "        optimizer_discriminators.zero_grad()\n",
    "        loss_discriminators = self.discriminators_step(T1, T2, \"training\")\n",
    "        loss_discriminators.backward()\n",
    "        optimizer_discriminators.step()\n",
    "        \n",
    "        return loss_generators + loss_discriminators\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        T1, T2 = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Entraînement des générateurs\n",
    "            loss_generators = self.generators_step(T1, T2, \"validation\")\n",
    "\n",
    "            # Entraînement des discriminateurs\n",
    "            loss_discriminators = self.discriminators_step(T1, T2, \"validation\")\n",
    "\n",
    "        return loss_generators + loss_discriminators\n",
    "\n",
    "    def on_train_start(self):\n",
    "        \"\"\"Code exécuté au début de l'entraînement.\"\"\"\n",
    "        string = f\"Version {self.trainer.logger.version}\"\n",
    "        print(f\"{string}\\n{'=' * len(string)}\\n\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Code exécuté à la fin de chaque époque d'entraînement.\"\"\"\n",
    "        metrics = self.trainer.callback_metrics\n",
    "\n",
    "        loss_gen_train = metrics[\"loss_generators_train\"].item()\n",
    "        loss_dis_train = metrics[\"loss_discriminators_train\"].item()\n",
    "        ssim_gen_train = metrics[\"ssim_generators_train\"].item()\n",
    "        ssim_dis_train = metrics[\"ssim_discriminators_train\"].item()\n",
    "\n",
    "        loss_gen_val = metrics[\"loss_generators_val\"].item()\n",
    "        loss_dis_val = metrics[\"loss_discriminators_val\"].item()\n",
    "        ssim_gen_val = metrics[\"ssim_generators_val\"].item()\n",
    "        ssim_dis_val = metrics[\"ssim_discriminators_val\"].item()\n",
    "\n",
    "        string = (f\"\"\"\n",
    "            Époque {self.trainer.current_epoch + 1} / {self.trainer.max_epochs}\n",
    "            -----------------------------------------------------------------\n",
    "            |              Jeu               | Fonction de perte |   SSIM   |\n",
    "            | ------------------------------ | ----------------- | -------- |\n",
    "            |   Entraînement (générateurs)   |{loss_gen_train:^19.5f}|{ssim_gen_train:^10.6f}|\n",
    "            | Entraînement (discriminateurs) |{loss_dis_train:^19.5f}|{ssim_dis_train:^10.6f}|\n",
    "            |    Validation (générateurs)    |{loss_gen_val:^19.5f}|{ssim_gen_val:^10.6f}|\n",
    "            |  Validation (discriminateurs)  |{loss_dis_val:^19.5f}|{ssim_dis_val:^10.6f}|\n",
    "            -----------------------------------------------------------------\n",
    "        \"\"\")\n",
    "        string = '\\n'.join([line.strip() for line in string.strip().split('\\n')])\n",
    "        print(string, \"\\n\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure les algorithmes d'optimisation à utiliser.\"\"\"\n",
    "        optimizer_generators = torch.optim.Adam(itertools.chain(\n",
    "            self.generator_from_t1_to_t2.parameters(), self.generator_from_t2_to_t1.parameters()\n",
    "        ), lr=self.lr, betas=self.betas)\n",
    "\n",
    "        optimizer_discriminators = torch.optim.Adam(itertools.chain(\n",
    "            self.discriminator_t1.parameters(), self.discriminator_t2.parameters()\n",
    "        ), lr=self.lr, betas=self.betas)\n",
    "\n",
    "        return [optimizer_generators, optimizer_discriminators]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee277ebb",
   "metadata": {},
   "source": [
    "On va maintenant entraîner notre modèle pendant $20$ époques.\n",
    "Pour les mêmes raisons que pour le cGAN, **on utilise un taux d'apprentissage plus faible**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "model_cyclegan = CycleGAN(lr=1e-4)\n",
    "\n",
    "trainer_cyclegan = L.Trainer(\n",
    "    max_epochs=20,\n",
    "    enable_model_summary=False,  # supprimer le résumé du modèle\n",
    "    logger=CSVLogger('.'),  # sauvegarder les résultats dans un fichier CSV\n",
    "    num_sanity_val_steps=0,  # ne pas effectuer d'étape de validation avant l'entraînement\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "\n",
    "trainer_cyclegan.fit(\n",
    "    model=model_cyclegan,\n",
    "    train_dataloaders=dataloader_train,\n",
    "    val_dataloaders=dataloader_val\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fc0f7f",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Visualizez quelques images générées par le générateur du CycleGAN grâce à la fonction `plot_generated_images()` définie plus haut.\n",
    "Comparez les performances du générateur du cGAN avec le générateur et expliquez la différence.\n",
    "Suggérez des pistes d'amélioration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ff4d7",
   "metadata": {},
   "source": [
    "**Réponse** : TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
